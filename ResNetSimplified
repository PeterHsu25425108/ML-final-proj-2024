{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\n# use process bar tool\nfrom tqdm import tqdm\nimport torchvision.models as models\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, Subset,Dataset\n# use process bar tool\nfrom tqdm import tqdm\nfrom PIL import Image\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-17T09:37:38.810900Z","iopub.execute_input":"2024-06-17T09:37:38.811149Z","iopub.status.idle":"2024-06-17T09:37:45.600211Z","shell.execute_reply.started":"2024-06-17T09:37:38.811125Z","shell.execute_reply":"2024-06-17T09:37:45.599449Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"class ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += identity\n        out = F.relu(out)\n\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T09:14:44.267583Z","iopub.execute_input":"2024-06-17T09:14:44.268049Z","iopub.status.idle":"2024-06-17T09:14:44.276415Z","shell.execute_reply.started":"2024-06-17T09:14:44.268019Z","shell.execute_reply":"2024-06-17T09:14:44.275405Z"}}},{"cell_type":"code","source":"# DEPTHWISE AND POINTWISE CONV\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResBlock, self).__init__()\n        self.depthwise_conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n        self.pointwise_conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        \n        self.depthwise_conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=out_channels, bias=False)\n        self.pointwise_conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.depthwise_conv1(x)\n        out = self.pointwise_conv1(out)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.depthwise_conv2(out)\n        out = self.pointwise_conv2(out)\n        out = self.bn2(out)\n\n        out += identity\n        out = F.relu(out)\n\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T09:37:45.601930Z","iopub.execute_input":"2024-06-17T09:37:45.602320Z","iopub.status.idle":"2024-06-17T09:37:45.611580Z","shell.execute_reply.started":"2024-06-17T09:37:45.602294Z","shell.execute_reply":"2024-06-17T09:37:45.610728Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=2):\n        super(ResNet, self).__init__()\n        self.in_channels = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:32:50.158157Z","iopub.execute_input":"2024-06-17T10:32:50.158512Z","iopub.status.idle":"2024-06-17T10:32:50.171475Z","shell.execute_reply.started":"2024-06-17T10:32:50.158481Z","shell.execute_reply":"2024-06-17T10:32:50.170595Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"aug_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    #transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(degrees=10), \n    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomResizedCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n])\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")  # Ensure the image is in RGB mode\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n    \ndef list_image_paths(directory):\n    return [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\ndef generate_augmented_dataset(original_dataset, num_augmented_images_per_class, transform):\n    augmented_images = []\n    augmented_labels = []\n    for label in [0, 1]:\n        counter = 0\n        while counter < num_augmented_images_per_class:\n            for img_path, lbl in zip(original_dataset.image_paths, original_dataset.labels):\n                if lbl == label:\n                    image = Image.open(img_path).convert(\"RGB\")\n                    augmented_img = transform(image)\n                    augmented_images.append(augmented_img)\n                    augmented_labels.append(lbl)\n                    counter += 1\n                    if counter >= num_augmented_images_per_class:\n                        break\n    return augmented_images, augmented_labels\n\n# List image paths for each class\nclass_0_image_paths = list_image_paths('/kaggle/input/ml-final-dataset/dataset/train/adults')\nclass_1_image_paths = list_image_paths('/kaggle/input/ml-final-dataset/dataset/train/children')\nclass_0_labels = [0] * len(class_0_image_paths)\nclass_1_labels = [1] * len(class_1_image_paths)\n\n# Combine both classes' image paths and labels\nall_image_paths = class_0_image_paths + class_1_image_paths\nall_labels = class_0_labels + class_1_labels\n\n# Create the original dataset\noriginal_dataset = CustomDataset(image_paths=all_image_paths, labels=all_labels, transform=None)\n\n# Number of augmented images generated per class\nnum_augmented_images_per_class = 6000\naugmented_images, augmented_labels = generate_augmented_dataset(original_dataset, num_augmented_images_per_class, aug_transforms)\n\n# Create a dataset from augmented images and labels\naugmented_dataset = [(img, lbl) for img, lbl in zip(augmented_images, augmented_labels)]\n\n# Custom dataset for augmented data\nclass AugmentedDataset(Dataset):\n    def __init__(self, augmented_data):\n        self.augmented_data = augmented_data\n\n    def __len__(self):\n        return len(self.augmented_data)\n\n    def __getitem__(self, idx):\n        image, label = self.augmented_data[idx]\n        return image, label\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:49:25.866299Z","iopub.execute_input":"2024-06-17T10:49:25.866950Z","iopub.status.idle":"2024-06-17T10:50:23.264345Z","shell.execute_reply.started":"2024-06-17T10:49:25.866916Z","shell.execute_reply":"2024-06-17T10:50:23.263371Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"batch_size = 200\n\n# Create dataset and DataLoader\naugmented_dataset = AugmentedDataset(augmented_dataset)\ntrain_loader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n\n# Define transformations for the dataset\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load dataset\n#train_dataset = datasets.ImageFolder(root='/kaggle/input/ml-final-dataset/dataset/train', transform=transform)\ntest_dataset = datasets.ImageFolder(root='/kaggle/input/ml-final-dataset/dataset/test', transform=transform)\n\n#train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:52:30.117831Z","iopub.execute_input":"2024-06-17T10:52:30.118223Z","iopub.status.idle":"2024-06-17T10:52:30.177259Z","shell.execute_reply.started":"2024-06-17T10:52:30.118193Z","shell.execute_reply":"2024-06-17T10:52:30.176372Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# = ResNet(ResBlock, [2, 2, 2, 2], num_classes=2)\n# Load the saved model\nmodel = torch.load('/kaggle/working/2024_06_17_ResNet.pt')\n\n# Transfer model to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:52:34.709561Z","iopub.execute_input":"2024-06-17T10:52:34.709913Z","iopub.status.idle":"2024-06-17T10:52:34.748951Z","shell.execute_reply.started":"2024-06-17T10:52:34.709884Z","shell.execute_reply":"2024-06-17T10:52:34.748185Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# 20\n# Training loop\nbest_model = model\nbest_acc = 75\n\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    \n    # Use tqdm to create a progress bar for the data loader\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\"):\n        images, labels = images.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n    \n    if(epoch%5 == 0):\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        if(100 * correct / total > best_acc):\n            best_acc = 100 * correct / total \n            best_model = model\n            print('Epoch = ',epoch)\n            print(f'Test Accuracy: {100 * correct / total:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:53:55.482413Z","iopub.execute_input":"2024-06-17T10:53:55.482795Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 60/60 [00:28<00:00,  2.13batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50], Loss: 0.2104\nEpoch =  0\nTest Accuracy: 75.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50: 100%|██████████| 60/60 [00:27<00:00,  2.22batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/50], Loss: 0.0923\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50: 100%|██████████| 60/60 [00:26<00:00,  2.23batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/50], Loss: 0.0570\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50: 100%|██████████| 60/60 [00:27<00:00,  2.21batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/50], Loss: 0.0576\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50: 100%|██████████| 60/60 [00:27<00:00,  2.20batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/50], Loss: 0.0430\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50: 100%|██████████| 60/60 [00:27<00:00,  2.21batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/50], Loss: 0.0395\nEpoch =  5\nTest Accuracy: 70.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50: 100%|██████████| 60/60 [00:27<00:00,  2.21batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/50], Loss: 0.0405\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50: 100%|██████████| 60/60 [00:27<00:00,  2.22batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/50], Loss: 0.0208\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50: 100%|██████████| 60/60 [00:26<00:00,  2.23batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/50], Loss: 0.0360\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50: 100%|██████████| 60/60 [00:26<00:00,  2.23batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/50], Loss: 0.0565\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50: 100%|██████████| 60/60 [00:26<00:00,  2.22batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/50], Loss: 0.0180\nEpoch =  10\nTest Accuracy: 73.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50: 100%|██████████| 60/60 [00:26<00:00,  2.23batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/50], Loss: 0.0110\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50: 100%|██████████| 60/60 [00:26<00:00,  2.22batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/50], Loss: 0.0104\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50: 100%|██████████| 60/60 [00:26<00:00,  2.22batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/50], Loss: 0.0174\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50:  78%|███████▊  | 47/60 [00:21<00:05,  2.21batch/s]","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the test dataset\n\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Test Accuracy: {100 * correct / total:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:47:43.262640Z","iopub.execute_input":"2024-06-17T10:47:43.263415Z","iopub.status.idle":"2024-06-17T10:47:44.022896Z","shell.execute_reply.started":"2024-06-17T10:47:43.263382Z","shell.execute_reply":"2024-06-17T10:47:44.021914Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Test Accuracy: 75.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install torchprofile torchsummary","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-17T10:04:50.580693Z","iopub.execute_input":"2024-06-17T10:04:50.581508Z","iopub.status.idle":"2024-06-17T10:05:04.472697Z","shell.execute_reply.started":"2024-06-17T10:04:50.581474Z","shell.execute_reply":"2024-06-17T10:05:04.471616Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Collecting torchprofile\n  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nRequirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (1.26.4)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (2.1.2)\nRequirement already satisfied: torchvision>=0.4 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (0.16.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.4->torchprofile) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.4->torchprofile) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4->torchprofile) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4->torchprofile) (1.3.0)\nDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary, torchprofile\nSuccessfully installed torchprofile-0.0.4 torchsummary-1.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchsummary import summary\nimport torchvision.models as models\nfrom torchprofile import profile_macs\n# Calculate the number of trainable parameters and all parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params = sum(p.numel() for p in model.parameters())\n\nprint('trainable_params = ', trainable_params)\nprint('all_params = ', all_params)\n\n# Create a dummy input tensor with the same size as the input images\ninput_tensor = torch.randn(1, 3, 224, 224).to(device)\n\n# Calculate FLOPs\nflops = profile_macs(model, input_tensor)\nprint(f'FLOPs: {flops / 1e9:.2f} GFLOPs')  # Convert to GFLOPs (GigaFLOPs)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:10:26.519741Z","iopub.execute_input":"2024-06-17T10:10:26.520352Z","iopub.status.idle":"2024-06-17T10:10:26.699843Z","shell.execute_reply.started":"2024-06-17T10:10:26.520317Z","shell.execute_reply":"2024-06-17T10:10:26.699022Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"trainable_params =  1443202\nall_params =  1443202\nFLOPs: 0.34 GFLOPs\n","output_type":"stream"}]},{"cell_type":"code","source":"# acc =75%\n#trainable_params =  1443202\n#all_params =  1443202\n#FLOPs: 0.34 GFLOPs\n# 6000 train data\n# 15 epoch\n#　batch_size = 200\n\n#torch.save(model, '/kaggle/working/2024_06_17_ResNet.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:05:12.546698Z","iopub.execute_input":"2024-06-17T10:05:12.547096Z","iopub.status.idle":"2024-06-17T10:05:12.588063Z","shell.execute_reply.started":"2024-06-17T10:05:12.547051Z","shell.execute_reply":"2024-06-17T10:05:12.587353Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}