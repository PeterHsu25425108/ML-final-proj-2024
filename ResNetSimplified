{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8662607,"sourceType":"datasetVersion","datasetId":5190382}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\n# use process bar tool\nfrom tqdm import tqdm\nimport torchvision.models as models\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, Subset,Dataset\n# use process bar tool\nfrom tqdm import tqdm\nfrom PIL import Image\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-18T04:41:34.535733Z","iopub.execute_input":"2024-06-18T04:41:34.536114Z","iopub.status.idle":"2024-06-18T04:41:38.096552Z","shell.execute_reply.started":"2024-06-18T04:41:34.536085Z","shell.execute_reply":"2024-06-18T04:41:38.095586Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"class ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += identity\n        out = F.relu(out)\n\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T09:14:44.267583Z","iopub.execute_input":"2024-06-17T09:14:44.268049Z","iopub.status.idle":"2024-06-17T09:14:44.276415Z","shell.execute_reply.started":"2024-06-17T09:14:44.268019Z","shell.execute_reply":"2024-06-17T09:14:44.275405Z"}}},{"cell_type":"code","source":"# DEPTHWISE AND POINTWISE CONV\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResBlock, self).__init__()\n        self.depthwise_conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n        self.pointwise_conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        \n        self.depthwise_conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=out_channels, bias=False)\n        self.pointwise_conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = self.depthwise_conv1(x)\n        out = self.pointwise_conv1(out)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.depthwise_conv2(out)\n        out = self.pointwise_conv2(out)\n        out = self.bn2(out)\n\n        out += identity\n        out = F.relu(out)\n\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:41:38.098098Z","iopub.execute_input":"2024-06-18T04:41:38.098529Z","iopub.status.idle":"2024-06-18T04:41:38.108409Z","shell.execute_reply.started":"2024-06-18T04:41:38.098503Z","shell.execute_reply":"2024-06-18T04:41:38.107324Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=2):\n        super(ResNet, self).__init__()\n        self.in_channels = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.fc(x)\n\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:41:38.617343Z","iopub.execute_input":"2024-06-18T04:41:38.618059Z","iopub.status.idle":"2024-06-18T04:41:38.631255Z","shell.execute_reply.started":"2024-06-18T04:41:38.618030Z","shell.execute_reply":"2024-06-18T04:41:38.630308Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class ModLayer4_ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=2):\n        super(ModLayer4_ResNet, self).__init__()\n        self.in_channels = 64\n        #self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.conv1_depth = nn.Conv2d(3, 3, kernel_size=7, stride=2, padding=3, groups=3, bias=False)\n        self.conv1_point = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n        \n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        \n        self.layer4_1 = nn.Conv2d(256, 256, kernel_size=3, padding=1, groups=256, bias=False)\n        self.layer4_2 = nn.Conv2d(256, 2, kernel_size=1, stride=1, bias=False)\n\n        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, num_classes)  # Adjusted to match the output of layer3\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels:\n            downsample = nn.Sequential(\n                #nn.Conv2d(self.in_channels, self.in_channels, kernel_size=3, padding=1, stride = stride,groups = self.in_channels, bias=False),\n                #nn.Conv2d(self.in_channels, out_channels, kernel_size=1, bias=False),\n                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels),\n            )\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        #x = self.conv1(x)\n        x=self.conv1_depth(x)\n        x = self.conv1_point(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        #x = self.avgpool(x)\n        #x = torch.flatten(x, 1)\n        #x = self.fc(x)\n\n        x = self.layer4_1(x)\n        x = self.layer4_2(x)\n        x = self.global_avgpool(x)\n        x = torch.flatten(x, 1)  # Flatten to (batch_size, 2)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:48:12.377470Z","iopub.execute_input":"2024-06-18T04:48:12.377871Z","iopub.status.idle":"2024-06-18T04:48:12.393008Z","shell.execute_reply.started":"2024-06-18T04:48:12.377842Z","shell.execute_reply":"2024-06-18T04:48:12.392058Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Clear previous variables from memory\nif 'class_0_image_paths' in locals():\n    del class_0_image_paths\nif 'class_1_image_paths' in locals():\n    del class_1_image_paths\n\nif 'class_0_labels' in locals():\n    del class_0_labels\nif 'class_1_labels' in locals():\n    del class_1_labels\n    \nif 'all_image_paths' in locals():\n    del all_image_paths\nif 'all_labels' in locals():\n    del all_labels\n    \nif 'original_dataset' in locals():\n    del original_dataset\n    \nif 'augmented_images' in locals():\n    del augmented_images\nif 'augmented_labels' in locals():\n    del augmented_labels\nif 'augmented_dataset' in locals():\n    print('del augmented_dataset')\n    del augmented_dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-18T05:26:35.917234Z","iopub.execute_input":"2024-06-18T05:26:35.917600Z","iopub.status.idle":"2024-06-18T05:26:35.922971Z","shell.execute_reply.started":"2024-06-18T05:26:35.917569Z","shell.execute_reply":"2024-06-18T05:26:35.922005Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"aug_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    #transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(degrees=10), \n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomResizedCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n])\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")  # Ensure the image is in RGB mode\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n    \ndef list_image_paths(directory):\n    return [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\ndef generate_augmented_dataset(original_dataset, num_augmented_images_per_class, transform):\n    augmented_images = []\n    augmented_labels = []\n    for label in [0, 1]:\n        counter = 0\n        while counter < num_augmented_images_per_class:\n            for img_path, lbl in zip(original_dataset.image_paths, original_dataset.labels):\n                if lbl == label:\n                    image = Image.open(img_path).convert(\"RGB\")\n                    augmented_img = transform(image)\n                    augmented_images.append(augmented_img)\n                    augmented_labels.append(lbl)\n                    counter += 1\n                    if counter >= num_augmented_images_per_class:\n                        break\n    return augmented_images, augmented_labels\n\n# List image paths for each class\n\nclass_0_image_paths = list_image_paths('/kaggle/input/ml-final-dataset/dataset/train/adults')\nclass_1_image_paths = list_image_paths('/kaggle/input/ml-final-dataset/dataset/train/children')\nclass_0_labels = [0] * len(class_0_image_paths)\nclass_1_labels = [1] * len(class_1_image_paths)\n\n# Combine both classes' image paths and labels\nall_image_paths = class_0_image_paths + class_1_image_paths\nall_labels = class_0_labels + class_1_labels\n\n# Create the original dataset\noriginal_dataset = CustomDataset(image_paths=all_image_paths, labels=all_labels, transform=None)\n\n# Number of augmented images generated per class\nnum_augmented_images_per_class = 6000\naugmented_images, augmented_labels = generate_augmented_dataset(original_dataset, num_augmented_images_per_class, aug_transforms)\n\n# Create a dataset from augmented images and labels\naugmented_dataset = [(img, lbl) for img, lbl in zip(augmented_images, augmented_labels)]\n\n# Custom dataset for augmented data\nclass AugmentedDataset(Dataset):\n    def __init__(self, augmented_data):\n        self.augmented_data = augmented_data\n\n    def __len__(self):\n        return len(self.augmented_data)\n\n    def __getitem__(self, idx):\n        image, label = self.augmented_data[idx]\n        return image, label\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T05:26:41.604177Z","iopub.execute_input":"2024-06-18T05:26:41.604795Z","iopub.status.idle":"2024-06-18T05:29:50.527626Z","shell.execute_reply.started":"2024-06-18T05:26:41.604761Z","shell.execute_reply":"2024-06-18T05:29:50.526461Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"batch_size = 150\n\n# Create dataset and DataLoader\naugmented_dataset = AugmentedDataset(augmented_dataset)\ntrain_loader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n\n# Define transformations for the dataset\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load dataset\n#train_dataset = datasets.ImageFolder(root='/kaggle/input/ml-final-dataset/dataset/train', transform=transform)\ntest_dataset = datasets.ImageFolder(root='/kaggle/input/ml-final-dataset/dataset/test', transform=transform)\n\n#train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T05:30:07.442696Z","iopub.execute_input":"2024-06-18T05:30:07.443362Z","iopub.status.idle":"2024-06-18T05:30:07.527504Z","shell.execute_reply.started":"2024-06-18T05:30:07.443327Z","shell.execute_reply":"2024-06-18T05:30:07.526770Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# = ResNet(ResBlock, [2, 2, 2, 2], num_classes=2)\n# Load the saved model\n#model = torch.load('/kaggle/working/2024_06_17_ResNetModLayer4.pt')\n\n# Instantiate the modified model\n#modified_model = ModLayer4_ResNet(ResBlock, [2, 2, 2, 2],num_classes=2)\n\n# Load the state dict of the original model\n#pretrained_dict = model.state_dict()\n\n# Filter out layer4 parameters from the state dict\n#filtered_dict = {k: v for k, v in pretrained_dict.items() if not( k.startswith('layer4') or k.startswith('fc'))}\n\n# Load the state dict into the modified model\n#modified_model.load_state_dict(filtered_dict , strict=False)\n#model = modified_model\n\nif(model != None):\n    del model\nmodel = torch.load('/kaggle/working/2024_06_17_ResNetModLayer4_SepConv7.pt')\n#model = ModLayer4_ResNet(ResBlock, [2, 2, 2, 2],num_classes=2)\n# Transfer model to GPU if available\ntorch.cuda.empty_cache()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T05:30:09.684321Z","iopub.execute_input":"2024-06-18T05:30:09.685048Z","iopub.status.idle":"2024-06-18T05:30:09.750140Z","shell.execute_reply.started":"2024-06-18T05:30:09.685017Z","shell.execute_reply":"2024-06-18T05:30:09.749429Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# 20\n# Training loop\n#best_model = model\n#best_acc = 75.83\n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    \n    # Use tqdm to create a progress bar for the data loader\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\"):\n        images, labels = images.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        del images\n        del labels\n        torch.cuda.empty_cache()\n    \n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-06-18T05:38:02.015471Z","iopub.execute_input":"2024-06-18T05:38:02.016102Z","iopub.status.idle":"2024-06-18T05:40:27.533532Z","shell.execute_reply.started":"2024-06-18T05:38:02.016070Z","shell.execute_reply":"2024-06-18T05:40:27.532618Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 80/80 [00:29<00:00,  2.75batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Loss: 0.0378\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 80/80 [00:29<00:00,  2.75batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/5], Loss: 0.0255\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 80/80 [00:29<00:00,  2.75batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/5], Loss: 0.0408\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 80/80 [00:29<00:00,  2.75batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/5], Loss: 0.0358\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 80/80 [00:29<00:00,  2.75batch/s]","output_type":"stream"},{"name":"stdout","text":"Epoch [5/5], Loss: 0.0331\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"    if(True):#epoch%3 == 0):\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        if(100 * correct / total > best_acc):\n            best_acc = 100 * correct / total \n            best_model = model\n            print('Epoch = ',epoch + 16)\n            print(f'Test Accuracy: {100 * correct / total:.2f}%')","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test dataset\n#model = best_model\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        #del images\n        #del labels\n        torch.cuda.empty_cache()\n\nprint(f'Test Accuracy: {100 * correct / total:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-18T05:40:31.262929Z","iopub.execute_input":"2024-06-18T05:40:31.263305Z","iopub.status.idle":"2024-06-18T05:40:31.798305Z","shell.execute_reply.started":"2024-06-18T05:40:31.263274Z","shell.execute_reply":"2024-06-18T05:40:31.797336Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Test Accuracy: 75.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"#torch.save(model, '/kaggle/working/2024_06_17_ResNet.pt')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T14:38:49.046072Z","iopub.execute_input":"2024-06-17T14:38:49.046723Z","iopub.status.idle":"2024-06-17T14:38:49.071190Z","shell.execute_reply.started":"2024-06-17T14:38:49.046692Z","shell.execute_reply":"2024-06-17T14:38:49.070155Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"%pip install torchprofile torchsummary","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-18T04:44:41.026604Z","iopub.execute_input":"2024-06-18T04:44:41.026972Z","iopub.status.idle":"2024-06-18T04:44:54.678334Z","shell.execute_reply.started":"2024-06-18T04:44:41.026943Z","shell.execute_reply":"2024-06-18T04:44:54.677235Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Collecting torchprofile\n  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nRequirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (1.26.4)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (2.1.2)\nRequirement already satisfied: torchvision>=0.4 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (0.16.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.4->torchprofile) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.4->torchprofile) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4->torchprofile) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4->torchprofile) (1.3.0)\nDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary, torchprofile\nSuccessfully installed torchprofile-0.0.4 torchsummary-1.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchsummary import summary\nimport torchvision.models as models\nfrom torchprofile import profile_macs\n# Calculate the number of trainable parameters and all parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params = sum(p.numel() for p in model.parameters())\n\nprint('trainable_params = ', trainable_params)\nprint('all_params = ', all_params)\n\n# Create a dummy input tensor with the same size as the input images\ninput_tensor = torch.randn(1, 3, 224, 224).to(device)\n\n# Calculate FLOPs\nflops = profile_macs(model, input_tensor)\nprint(f'FLOPs: {flops / 1e9:.2f} GFLOPs')  # Convert to GFLOPs (GigaFLOPs)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T05:03:43.262259Z","iopub.execute_input":"2024-06-18T05:03:43.262988Z","iopub.status.idle":"2024-06-18T05:03:43.415515Z","shell.execute_reply.started":"2024-06-18T05:03:43.262956Z","shell.execute_reply":"2024-06-18T05:03:43.414510Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"trainable_params =  366613\nall_params =  366613\nFLOPs: 0.17 GFLOPs\n","output_type":"stream"}]},{"cell_type":"code","source":"# acc =75.83 %\n#trainable_params =  1443202\n#all_params =  1443202\n#FLOPs: 0.34 GFLOPs\n# 12000 train data\n# 18 epoch\n#　batch_size = 200\n\n#torch.save(model, '/kaggle/working/2024_06_17_ResNet.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:05:12.546698Z","iopub.execute_input":"2024-06-17T10:05:12.547096Z","iopub.status.idle":"2024-06-17T10:05:12.588063Z","shell.execute_reply.started":"2024-06-17T10:05:12.547051Z","shell.execute_reply":"2024-06-17T10:05:12.587353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# acc =77.5%\n#trainable_params =  375682\n#all_params =  375682\n#FLOPs: 0.29 GFLOPs\n# 12000 train data\n# 5 epoch\n#　batch_size = 200\n#torch.save(model, '/kaggle/working/2024_06_17_ResNetModLayer4.pt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#acc = 75%\n#trainable_params =  366613\n#all_params =  366613\n#FLOPs: 0.17 GFLOPs\n# 6000 train data\n# 30 epoch\n# batch_size = 100\n#torch.save(model, '/kaggle/working/2024_06_17_ResNetModLayer4_SepConv7.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-18T05:09:56.099405Z","iopub.execute_input":"2024-06-18T05:09:56.099757Z","iopub.status.idle":"2024-06-18T05:09:56.124302Z","shell.execute_reply.started":"2024-06-18T05:09:56.099728Z","shell.execute_reply":"2024-06-18T05:09:56.123495Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}