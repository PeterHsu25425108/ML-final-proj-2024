{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8662607,"sourceType":"datasetVersion","datasetId":5190382}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\n# use process bar tool\nfrom tqdm import tqdm\nimport torchvision.models as models\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, Subset,Dataset\n# use process bar tool\nfrom tqdm import tqdm\nfrom PIL import Image\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-06-17T05:19:05.915105Z","iopub.execute_input":"2024-06-17T05:19:05.915959Z","iopub.status.idle":"2024-06-17T05:19:05.923028Z","shell.execute_reply.started":"2024-06-17T05:19:05.915924Z","shell.execute_reply":"2024-06-17T05:19:05.921988Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"aug_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    #transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(degrees=10), \n    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomResizedCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n])\n\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")  # Ensure the image is in RGB mode\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n    \ndef list_image_paths(directory):\n    return [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\ndef generate_augmented_dataset(original_dataset, num_augmented_images_per_class, transform):\n    augmented_images = []\n    augmented_labels = []\n    for label in [0, 1]:\n        counter = 0\n        while counter < num_augmented_images_per_class:\n            for img_path, lbl in zip(original_dataset.image_paths, original_dataset.labels):\n                if lbl == label:\n                    image = Image.open(img_path).convert(\"RGB\")\n                    augmented_img = transform(image)\n                    augmented_images.append(augmented_img)\n                    augmented_labels.append(lbl)\n                    counter += 1\n                    if counter >= num_augmented_images_per_class:\n                        break\n    return augmented_images, augmented_labels\n\n# List image paths for each class\nclass_0_image_paths = list_image_paths('/kaggle/input/ml-final-dataset/dataset/train/adults')\nclass_1_image_paths = list_image_paths('/kaggle/input/ml-final-dataset/dataset/train/children')\nclass_0_labels = [0] * len(class_0_image_paths)\nclass_1_labels = [1] * len(class_1_image_paths)\n\n# Combine both classes' image paths and labels\nall_image_paths = class_0_image_paths + class_1_image_paths\nall_labels = class_0_labels + class_1_labels\n\n# Create the original dataset\noriginal_dataset = CustomDataset(image_paths=all_image_paths, labels=all_labels, transform=None)\n\n# Number of augmented images generated per class\nnum_augmented_images_per_class = 720\naugmented_images, augmented_labels = generate_augmented_dataset(original_dataset, num_augmented_images_per_class, aug_transforms)\n\n# Create a dataset from augmented images and labels\naugmented_dataset = [(img, lbl) for img, lbl in zip(augmented_images, augmented_labels)]\n\n# Custom dataset for augmented data\nclass AugmentedDataset(Dataset):\n    def __init__(self, augmented_data):\n        self.augmented_data = augmented_data\n\n    def __len__(self):\n        return len(self.augmented_data)\n\n    def __getitem__(self, idx):\n        image, label = self.augmented_data[idx]\n        return image, label\n\n# Create dataset and DataLoader\naugmented_dataset = AugmentedDataset(augmented_dataset)\ntrain_loader = DataLoader(augmented_dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T05:19:07.679951Z","iopub.execute_input":"2024-06-17T05:19:07.680689Z","iopub.status.idle":"2024-06-17T05:19:14.436234Z","shell.execute_reply.started":"2024-06-17T05:19:07.680653Z","shell.execute_reply":"2024-06-17T05:19:14.435231Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define transformations for the dataset\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load dataset\n#train_dataset = datasets.ImageFolder(root='/kaggle/input/ml-final-dataset/dataset/train', transform=transform)\ntest_dataset = datasets.ImageFolder(root='/kaggle/input/ml-final-dataset/dataset/test', transform=transform)\n\n#train_loader = DataLoader(dataset=train_dataset, batch_size=56, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=56, shuffle=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-17T05:19:14.783330Z","iopub.execute_input":"2024-06-17T05:19:14.783705Z","iopub.status.idle":"2024-06-17T05:19:14.792586Z","shell.execute_reply.started":"2024-06-17T05:19:14.783676Z","shell.execute_reply":"2024-06-17T05:19:14.791626Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class ModifiedInceptionModule(nn.Module):\n    def __init__(self, in_channels):\n        super(ModifiedInceptionModule, self).__init__()\n\n        self.conv_1x1 = nn.Conv2d(in_channels, in_channels//2, kernel_size=1)\n        self.depthwise_3x3 = nn.Conv2d(in_channels, in_channels//2, kernel_size=3, padding=1)\n        self.pointwise_3x3 = nn.Conv2d(in_channels//2, in_channels//2, kernel_size=1)\n\n    def forward(self, x):\n        path1 = self.conv_1x1(x)\n        path2 = self.depthwise_3x3(x)\n        path2 = self.pointwise_3x3(path2)\n        outputs = torch.cat([path1, path2], dim=1)\n        #outputs = outputs[:, :x.size(1), :, :]  # Keep the same number of channels as the input\n        outputs += x\n        return F.relu(outputs)\n\nclass DeepNetwork(nn.Module):\n    def __init__(self, num_classes=2, num_inception=5):\n        super(DeepNetwork, self).__init__()\n\n        self.initial_conv = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n        self.downsample = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n\n        #self.inception1 = ModifiedInceptionModule(32)\n        #self.inception2 = ModifiedInceptionModule(32)\n        #self.inception3 = ModifiedInceptionModule(32)\n        #self.inception4 = ModifiedInceptionModule(32)\n        self.inception = nn.ModuleList([ModifiedInceptionModule(64) for _ in range(num_inception)])\n        for i in range(num_inception):\n            self.inception.append(ModifiedInceptionModule(64))\n\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.initial_conv(x))\n        x = F.relu(self.downsample(x))\n        \n        #x = self.inception1(x)\n        #x = self.inception2(x)\n        #x = self.inception3(x)\n        #x = self.inception4(x)\n        for i in range(len(self.inception)):\n            x = self.inception[i](x)\n            \n        x = self.global_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T05:19:18.025227Z","iopub.execute_input":"2024-06-17T05:19:18.025590Z","iopub.status.idle":"2024-06-17T05:19:18.038176Z","shell.execute_reply.started":"2024-06-17T05:19:18.025565Z","shell.execute_reply":"2024-06-17T05:19:18.037099Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Model, loss function, optimizer\nmodel = DeepNetwork(num_classes=2, num_inception = 5)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Transfer model to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    \n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T05:19:21.732571Z","iopub.execute_input":"2024-06-17T05:19:21.733335Z","iopub.status.idle":"2024-06-17T05:19:48.059134Z","shell.execute_reply.started":"2024-06-17T05:19:21.733296Z","shell.execute_reply":"2024-06-17T05:19:48.058157Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 0.6893\nEpoch [2/10], Loss: 0.6696\nEpoch [3/10], Loss: 0.6630\nEpoch [4/10], Loss: 0.6585\nEpoch [5/10], Loss: 0.6677\nEpoch [6/10], Loss: 0.6606\nEpoch [7/10], Loss: 0.6535\nEpoch [8/10], Loss: 0.6672\nEpoch [9/10], Loss: 0.6625\nEpoch [10/10], Loss: 0.6569\n","output_type":"stream"}]},{"cell_type":"code","source":"# Testing loop\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Test Accuracy: {100 * correct / total:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-17T05:19:54.098901Z","iopub.execute_input":"2024-06-17T05:19:54.099846Z","iopub.status.idle":"2024-06-17T05:19:55.447270Z","shell.execute_reply.started":"2024-06-17T05:19:54.099812Z","shell.execute_reply":"2024-06-17T05:19:55.446347Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Test Accuracy: 60.83%\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Train Accuracy: {100 * correct / total:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-16T07:06:14.960167Z","iopub.execute_input":"2024-06-16T07:06:14.960575Z","iopub.status.idle":"2024-06-16T07:06:18.166783Z","shell.execute_reply.started":"2024-06-16T07:06:14.960544Z","shell.execute_reply":"2024-06-16T07:06:18.165880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install torchsummary torchprofile","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-16T12:56:24.402395Z","iopub.execute_input":"2024-06-16T12:56:24.403031Z","iopub.status.idle":"2024-06-16T12:56:37.644045Z","shell.execute_reply.started":"2024-06-16T12:56:24.402998Z","shell.execute_reply":"2024-06-16T12:56:37.642883Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nCollecting torchprofile\n  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\nRequirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (1.26.4)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (2.1.2)\nRequirement already satisfied: torchvision>=0.4 in /opt/conda/lib/python3.10/site-packages (from torchprofile) (0.16.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.4->torchprofile) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.4->torchprofile) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4->torchprofile) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.4->torchprofile) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4->torchprofile) (1.3.0)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\nInstalling collected packages: torchsummary, torchprofile\nSuccessfully installed torchprofile-0.0.4 torchsummary-1.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchsummary import summary\nimport torchvision.models as models\nfrom torchprofile import profile_macs\n# Calculate the number of trainable parameters and all parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params = sum(p.numel() for p in model.parameters())\n\nprint('trainable_params = ', trainable_params)\nprint('all_params = ', all_params)\n\n# Create a dummy input tensor with the same size as the input images\ninput_tensor = torch.randn(1, 3, 224, 224).to(device)\n\n# Calculate FLOPs\nflops = profile_macs(model, input_tensor)\nprint(f'FLOPs: {flops / 1e9:.2f} GFLOPs')  # Convert to GFLOPs (GigaFLOPs)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:56:37.646323Z","iopub.execute_input":"2024-06-16T12:56:37.646667Z","iopub.status.idle":"2024-06-16T12:56:37.966653Z","shell.execute_reply.started":"2024-06-16T12:56:37.646637Z","shell.execute_reply":"2024-06-16T12:56:37.965656Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"trainable_params =  470850\nall_params =  470850\nFLOPs: 1.49 GFLOPs\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, '/kaggle/working/CustomModel0616.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}